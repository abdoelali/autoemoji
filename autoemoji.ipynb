{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "import gspread\n",
    "import matplotlib.pyplot as plt\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from skimage import color, exposure, feature, io, transform\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# setup_run = True\n",
    "# data_base_path = 'https://ait.ethz.ch/public-data/computational_interaction2016/'\n",
    "\n",
    "# if not os.path.exists('train'):\n",
    "#     print('[INFO]: Looks like you do not have training data. Let me fetch that for you.')\n",
    "#     sys.stdout.flush()\n",
    "#     url_traindata = data_base_path+'train.zip'\n",
    "#     filename = wget.download(url_traindata)\n",
    "#     zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "#     zip_ref.extractall('./')\n",
    "#     zip_ref.close()\n",
    "#     print('[INFO]: Training data fetching completed.')\n",
    "#     sys.stdout.flush()\n",
    "    \n",
    "# if not os.path.exists('./test_T30_R60'):\n",
    "#     print('[INFO]: Looks like you do not have testing data. Let me fetch that for you')\n",
    "#     sys.stdout.flush()\n",
    "#     url_testdata = data_base_path+'test_T30_R60.zip'\n",
    "#     filename = wget.download(url_testdata)\n",
    "#     zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "#     zip_ref.extractall('./')\n",
    "#     zip_ref.close()\n",
    "#     print('[INFO]: Testing data fetching completed.')\n",
    "#     sys.stdout.flush()\n",
    "    \n",
    "# Additionally, there's a second, more challenging dataset that you can download from \n",
    "# url_testdata_hard = 'https://ait.inf.ethz.ch/teaching/courses/2016-SS-User-Interface-Engineering/downloads/exercises/test_T30_R90.zip '\n",
    "    \n",
    "# Compute accuracy, precision, recall and confusion matrix and (optionally) prints them on screen\n",
    "def compute_scores(y_pred, y_true, verbose=False):\n",
    "\n",
    "    hits = 0\n",
    "    for p in range(1,len(y_true)):\n",
    "        if y_pred[p] == y_true[p]:\n",
    "            hits += 1\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if(verbose):\n",
    "        print (\"(RW) Accuracy: \" + str(accuracy) + \"(\" + str(hits) + \"/\" + str(len(y_true)) + \")\")\n",
    "        print (\"Precision: \" + str(precision))\n",
    "        print (\"Recall: \" + str(recall))\n",
    "        print (\"Confusion Matrix\")\n",
    "        print (conf_mat)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "# Extract HOG features from an image and (optionally) show the features superimposed on it \n",
    "def extractHOG(inputimg, showHOG=False): \n",
    "    \n",
    "    # convert image to single-channel, grayscale\n",
    "    image = color.rgb2gray(inputimg)\n",
    "\n",
    "    #extract HOG features\n",
    "    if showHOG:\n",
    "        fd, hog_image = feature.hog(image, orientations=36, \n",
    "                                    pixels_per_cell=(16, 16),\n",
    "                                    cells_per_block=(2, 2), \n",
    "                                    visualise=showHOG)\n",
    "    else:\n",
    "        fd = feature.hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                         cells_per_block=(1, 1), visualise=showHOG)\n",
    "    if(showHOG):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "        ax1.set_adjustable('box-forced')\n",
    "        # Rescale histogram for better display\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 0.02))\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "        ax1.set_adjustable('box-forced')\n",
    "        plt.show()\n",
    "    return fd\n",
    "\n",
    "\n",
    "# Load a dataset (Data, Labels) from a folder.\n",
    "# Return data (HOGs, Class) and image list (as image file ames on disk)\n",
    "def load_dataset_from_folder(root_folder, rgb_folder, segmentation_folder):\n",
    "            \n",
    "    HOGs_list = []\n",
    "    Cs_list = []    \n",
    "    image_list = []\n",
    "    if os.path.exists(root_folder):\n",
    "        class_folders = next(os.walk(root_folder))[1]\n",
    "        class_folders.sort()\n",
    "        print(\"[INFO] Found \" + str(len(class_folders)) + \" class folders\")\n",
    "        print(class_folders)\n",
    "        sys.stdout.flush()\n",
    "        tot_classes = len(class_folders)\n",
    "        #used to resize the images\n",
    "        image_size = (128, 128)\n",
    "        class_list = range(tot_classes)\n",
    "        for class_folder,this_class in zip(class_folders,class_list):\n",
    "            print(\"\\n[INFO] Processing folder \" + class_folder)\n",
    "            sys.stdout.flush()\n",
    "            current_gesture_folder_rgb = root_folder + class_folder + \"/\" + rgb_folder + \"/*.jpg\"\n",
    "            current_gesture_folder_segmentation = root_folder + class_folder + \"/\" + segmentation_folder + \"/*.png\"\n",
    "            allfiles_imgs = glob.glob(current_gesture_folder_rgb)\n",
    "            allfiles_masks = glob.glob(current_gesture_folder_segmentation)\n",
    "            #for each image/mask pair\n",
    "            line_percentage_cnt = 0\n",
    "            for file_img,mask_img in zip(allfiles_imgs,allfiles_masks):\n",
    "                # Print completion percentage\n",
    "                sys.stdout.write('\\r')\n",
    "                progress_bar_msg = \"[%-100s] %d%% \" + str(line_percentage_cnt) + \"/\" + str(len(allfiles_imgs))\n",
    "                update_step = int( (float(100)/float(len(allfiles_imgs))) * float(line_percentage_cnt) )\n",
    "                sys.stdout.write(progress_bar_msg % ('='*update_step, update_step))\n",
    "                sys.stdout.flush()\n",
    "                img = io.imread(file_img)\n",
    "                mask = io.imread(mask_img)\n",
    "                mask = 255 - mask\n",
    "                img *= mask\n",
    "                # you can see the segmented image using:\n",
    "                #io.imshow(img)\n",
    "                #io.show()\n",
    "                \n",
    "                feat = extractHOG(transform.resize(img, image_size))\n",
    "                HOGs_list.append(feat)\n",
    "                Cs_list.append(this_class)\n",
    "                image_list.append(file_img)\n",
    "                line_percentage_cnt += 1\n",
    "        print(\"[INFO] Loaded data in. Number of samples: \"+ str(len(image_list)))\n",
    "    else:\n",
    "        print(\"[ERROR] Folder \" + root_folder + \" does not exist!\")\n",
    "        print(\"[ERROR] Have you run the setup cell?\")\n",
    "        sys.stdout.flush()\n",
    "        exit()\n",
    "        \n",
    "\n",
    "    HOGs = np.array(HOGs_list)\n",
    "    Cs = np.array(Cs_list)\n",
    "    return HOGs, Cs, image_list\n",
    "\n",
    "\n",
    "        \n",
    "# Class to store parameters of an SVM\n",
    "class SVMparameters:\n",
    "\n",
    "    def __init__(self, k='rbf', c='1', g='0.1', d=1):\n",
    "        self.kernel = k\n",
    "        self.C = c\n",
    "        self.gamma=g\n",
    "        self.degree = g\n",
    "\n",
    "    def setkernel(self, k):\n",
    "        self.kernel = k\n",
    "\n",
    "    def setgamma(self, g):\n",
    "        self.gamma = g\n",
    "\n",
    "    def setc(self, c):\n",
    "        self.C = c\n",
    "\n",
    "    def setdegree(self,d):\n",
    "        self.degree = d\n",
    "    \n",
    "    def printconfig(self):\n",
    "        print(\"Kernel: \" + self.kernel)\n",
    "        if self.kernel is \"poly\":\n",
    "            print(\"Degree: \" + str(self.degree))\n",
    "        print(\"C: \" + str(self.C))\n",
    "        print(\"Gamma: \" + str(self.gamma))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 141 class folders\n"
     ]
    }
   ],
   "source": [
    "train_folders = next(os.walk(\"KDEF/\"))[1]\n",
    "train_folders.sort()\n",
    "print(\"[INFO] Found \" + str(len(train_folders)) + \" class folders\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "image_list = []\n",
    "HOGs_list = []\n",
    "Cs_list = [] \n",
    "\n",
    "image_size = (128, 128)\n",
    "                \n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "\n",
    "emotion_list = ['AFS.JPG', 'ANS.JPG', 'DIS.JPG', 'HAS.JPG', 'NES.JPG', 'SAS.JPG', 'SUS.JPG']\n",
    "\n",
    "emotion_num = [0,1,2,3,4,5,6]\n",
    "\n",
    "ffile = []\n",
    "line_percentage_cnt = 0\n",
    "\n",
    "for class_folder,this_class in zip(train_folders,class_list):\n",
    "#     print(\"\\n[INFO] Processing folder \" + class_folder)\n",
    "    sys.stdout.flush()\n",
    "  \n",
    "    \n",
    "    ffile = os.listdir('KDEF/' + class_folder)\n",
    "\n",
    "    for i in ffile:\n",
    "        \n",
    "        for j in range(len(emotion_list)):\n",
    "\n",
    "            if i.endswith(emotion_list[j]): #afraid\n",
    "                \n",
    "                Cs_list.append(emotion_num[j])\n",
    "                \n",
    "                print Cs_list\n",
    "                \n",
    "                directory = 'KDEF/train/' + emotion_list[j][0:3] \n",
    "                \n",
    "                RGB_subdir = 'RGB/'\n",
    "                segmented_subdir = 'segmented/'\n",
    "                \n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                     \n",
    "                if not os.path.exists(directory + \"/\" + RGB_subdir):\n",
    "                    os.makedirs(directory + \"/\" + RGB_subdir)\n",
    "                \n",
    "              \n",
    "                tmp_img = io.imread(\"KDEF/\" + class_folder + \"/\" + i)\n",
    "\n",
    "                \n",
    "            \n",
    "#                 sys.stdout.write('\\r')\n",
    "#                 progress_bar_msg = \"[%-100s] %d%% \" + str(line_percentage_cnt) + \"/\" + str(len(class_folder))\n",
    "#                 update_step = int( (float(100)/float(len(allfiles_imgs))) * float(line_percentage_cnt) )\n",
    "#                 sys.stdout.write(progress_bar_msg % ('='*update_step, update_step))\n",
    "#                 sys.stdout.flush()\n",
    "\n",
    "#                 feat = extractHOG(img_resize)\n",
    "        \n",
    "#                 HOGs_list.append(feat)\n",
    "    #             Cs_list.append(this_class)\n",
    "#                 image_list.append(tmp_img)\n",
    "\n",
    "#                 img = cv2.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "                gray = cv2.cvtColor(tmp_img,cv2.COLOR_BGR2GRAY)\n",
    "                ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        \n",
    "        \n",
    "                ###########################\n",
    "   \n",
    "                faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "                for (x,y,w,h) in faces:\n",
    "                    cv2.rectangle(tmp_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                    roi_gray = gray[y:y+h, x:x+w]\n",
    "                    roi_color = tmp_img[y:y+h, x:x+w]\n",
    "                    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "#                     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "                    for (ex,ey,ew,eh) in eyes:\n",
    "                        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "                img_resize = tmp_img[y:y+h,x:x+w]\n",
    "\n",
    "                \n",
    "                io.imsave(directory + \"/\" + RGB_subdir + i, img_resize)\n",
    "                \n",
    "                ##########################\n",
    "\n",
    "# #                 # noise removal\n",
    "#                 kernel = np.ones((3,3),np.uint8)\n",
    "#                 opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "# #                 # sure background area\n",
    "#                 sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "\n",
    "# #                 # Finding sure foreground area\n",
    "#                 dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "#                 ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "\n",
    "# #                 # Finding unknown region\n",
    "#                 sure_fg = np.uint8(sure_fg)\n",
    "#                 mask_img = cv2.subtract(sure_bg,sure_fg)\n",
    "\n",
    "#                 if not os.path.exists(directory + \"/\" + segmented_subdir):\n",
    "#                     os.makedirs(directory + \"/\" + segmented_subdir)\n",
    "                    \n",
    "# #               #  cv2.imwrite('KDEF/test.png', mask_img)\n",
    "#                 io.imsave(directory + \"/\" + segmented_subdir + i, mask_img)\n",
    "\n",
    "#                 mask = 255 - mask_img\n",
    "#                 gray *= mask \n",
    "                feat = extractHOG(transform.resize(gray, image_size))\n",
    "\n",
    "                HOGs_list.append(feat)\n",
    "#                 Cs_list.append(this_class)\n",
    "                image_list.append(img_resize)\n",
    "                \n",
    "\n",
    "        \n",
    "HOGs = np.array(HOGs_list)\n",
    "Cs = np.array(Cs_list)\n",
    "\n",
    "\n",
    "    \n",
    "#     return HOGs, Cs, image_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980,)\n",
      "(13160,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print Cs.shape\n",
    "print feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 379, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "# mouth_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_smile.xml')\n",
    "\n",
    "\n",
    "img = cv2.imread('KDEF/AF01/AF01AFS.JPG')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "#     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    \n",
    "#     for (ex,ey,ew,eh) in mouth:\n",
    "#         cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "        \n",
    "img_resize = img[y:y+h,x:x+w]\n",
    "\n",
    "\n",
    "print img_resize.shape\n",
    "\n",
    "# io.imshow(img_resize)\n",
    "# io.show()\n",
    "\n",
    "# cv2.imshow('img',img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import xfeatures2d\n",
    "\n",
    "xfeatures2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762, 562)\n",
      "(762, 562)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.14695973,  0.1065591 ,  0.06730217, ...,  0.08589244,\n",
       "        0.11436558,  0.10563707])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_size = (128, 128)\n",
    "# img = cv2.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "\n",
    "img = cv2.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "# noise removal\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "\n",
    "# Finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "\n",
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "mask_img = cv2.subtract(sure_bg,sure_fg)\n",
    "\n",
    "cv2.imwrite('KDEF/test.png', mask_img)\n",
    "\n",
    "# io.imshow(mask_img)\n",
    "# io.show()\n",
    "\n",
    "# img = io.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "mask = io.imread(\"KDEF/test.png\")\n",
    "mask = 255 - mask\n",
    "\n",
    "# print mask.shape\n",
    "# print gray.shape\n",
    "# (762, 562, 3)\n",
    "# 388 rows, 647 columns, and 3 channels (the RGB components).\n",
    "\n",
    "gray *= mask\n",
    "feat = extractHOG(transform.resize(img, image_size))\n",
    "extractHOG(img, showHOG=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = io.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "mask = io.imread(mask_img\n",
    "mask = 255 - mask\n",
    "img *= mask\n",
    "# you can see the segmented image using:\n",
    "#io.imshow(img)\n",
    "#io.show()\n",
    "feat = extractHOG(transform.resize(img, image_size))\n",
    "\n",
    "extractHOG(img, showHOG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from window import window_data\n",
    "\n",
    "## The skeleton of a solution\n",
    "\n",
    "import window\n",
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from IPython.core.display import HTML\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process\n",
    "\n",
    "# - get external labeled facial images from available datasets\n",
    "# - image database: extract relevant features, e.g., color histograms, contours, etc. (use off shelf packages)\n",
    "# - features split into training and test data\n",
    "# - classifier must be trained on extracted training features\n",
    "\n",
    "# - Voice input: “emoji” command, starts window function (e.g., 200 ms), draws randomly sampled snapshot\n",
    "# - snapshot used as input to trained classifier (e.g., scikit.svm.predict(snapshot)). returns label (happy, sad, neutral, angry)\n",
    "# - get label and input image, and convert image to cartoon. store final cartoon image into database, with associated label. this gives personalized emoji database\n",
    "# - user says “emoji”, and if facial features matches existing emoji in database\n",
    "# - two windows, one with your face and other with output emoji\n",
    "\n",
    "# corner features, sift features, gabor features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load the wave file and normalise\n",
    "# load \"data/rub_1.wav\" and \"data/rub_2.wav\"\n",
    "\n",
    "def load_wave(fname):\n",
    "    # load and return a wave file\n",
    "    sr, wave = scipy.io.wavfile.read(fname)\n",
    "    return wave/32768.0\n",
    "\n",
    "\n",
    "\n",
    "def breath_classify():\n",
    "\n",
    "    rub_1 = load_wave(\"data/in.wav\")[:,0]\n",
    "    rub_2 = load_wave(\"data/out.wav\")[:,0]\n",
    "\n",
    "    rub_1_features = window.window_data(rub_1, 120)\n",
    "    rub_2_features = window.window_data(rub_2, 120)\n",
    "\n",
    "    rub_1_labels = np.zeros(len(rub_1_features,))\n",
    "    rub_2_labels = np.ones(len(rub_2_features,))\n",
    "\n",
    "    rub_features = np.vstack([rub_1_features, rub_2_features])\n",
    "    rub_labels = np.hstack([rub_1_labels, rub_2_labels])\n",
    "    print rub_features.shape, rub_labels.shape\n",
    "\n",
    "    rubfft_features =  np.abs(np.fft.fft(rub_features))\n",
    "    rubfft_train_features, rubfft_test_features, rub_train_labels, rub_test_labels = sklearn.cross_validation.train_test_split(\n",
    "        rubfft_features, rub_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "    rub_train_features, rub_test_features, rub_train_labels, rub_test_labels = sklearn.cross_validation.train_test_split(\n",
    "        rub_features, rub_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "    print rub_train_features.shape, rub_train_labels.shape\n",
    "\n",
    "    svm = sklearn.svm.SVC(gamma=0.1, C=100)\n",
    "    svm.fit(rub_train_features, rub_train_labels)\n",
    "\n",
    "    # we can plot the receiver-operator curve: the graph of false positive rate against true positive rate\n",
    "    # scores = svm.decision_function(rub_test_features)\n",
    "    # print scores.shape, rub_test_labels.shape\n",
    "    # fpr, tpr, thresholds = sklearn.metrics.roc_curve(rub_test_labels, scores)\n",
    "    # plt.plot(fpr,tpr)\n",
    "    # plt.plot([0,1], [0,1])\n",
    "    # plt.plot([0,1], [1,0])\n",
    "    # plt.fill_between(fpr, tpr, facecolor='none', hatch='/', alpha=0.2)\n",
    "    # plt.xlabel(\"False positive rate\")\n",
    "    # plt.ylabel(\"True positive rate\")\n",
    "    # plt.legend([\"ROC\", \"Chance\", \"EER line\"])\n",
    "\n",
    "    # print svm\n",
    "\n",
    "    return svm\n",
    "\n",
    "\n",
    "breath_classify()\n",
    " # takes feature vector and returns a label\n",
    "\n",
    "\n",
    "# split into windows\n",
    "\n",
    "# test/train split\n",
    "\n",
    "# train classifier\n",
    "\n",
    "# evaluate classifier\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
