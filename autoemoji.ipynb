{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 61,
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "\n",
    "### Required initial run, many functions and imports needed for later ###\n",
    "\n",
=======
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "import gspread\n",
    "import matplotlib.pyplot as plt\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from skimage import color, exposure, feature, io, transform\n",
    "\n",
<<<<<<< HEAD
    "import pickle\n",
=======
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "import numpy as np\n",
    "import cv2\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# setup_run = True\n",
    "# data_base_path = 'https://ait.ethz.ch/public-data/computational_interaction2016/'\n",
    "\n",
    "# if not os.path.exists('train'):\n",
    "#     print('[INFO]: Looks like you do not have training data. Let me fetch that for you.')\n",
    "#     sys.stdout.flush()\n",
    "#     url_traindata = data_base_path+'train.zip'\n",
    "#     filename = wget.download(url_traindata)\n",
    "#     zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "#     zip_ref.extractall('./')\n",
    "#     zip_ref.close()\n",
    "#     print('[INFO]: Training data fetching completed.')\n",
    "#     sys.stdout.flush()\n",
    "    \n",
    "# if not os.path.exists('./test_T30_R60'):\n",
    "#     print('[INFO]: Looks like you do not have testing data. Let me fetch that for you')\n",
    "#     sys.stdout.flush()\n",
    "#     url_testdata = data_base_path+'test_T30_R60.zip'\n",
    "#     filename = wget.download(url_testdata)\n",
    "#     zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "#     zip_ref.extractall('./')\n",
    "#     zip_ref.close()\n",
    "#     print('[INFO]: Testing data fetching completed.')\n",
    "#     sys.stdout.flush()\n",
    "    \n",
    "# Additionally, there's a second, more challenging dataset that you can download from \n",
    "# url_testdata_hard = 'https://ait.inf.ethz.ch/teaching/courses/2016-SS-User-Interface-Engineering/downloads/exercises/test_T30_R90.zip '\n",
    "    \n",
    "# Compute accuracy, precision, recall and confusion matrix and (optionally) prints them on screen\n",
    "def compute_scores(y_pred, y_true, verbose=False):\n",
    "\n",
    "    hits = 0\n",
    "    for p in range(1,len(y_true)):\n",
    "        if y_pred[p] == y_true[p]:\n",
    "            hits += 1\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if(verbose):\n",
    "        print (\"(RW) Accuracy: \" + str(accuracy) + \"(\" + str(hits) + \"/\" + str(len(y_true)) + \")\")\n",
    "        print (\"Precision: \" + str(precision))\n",
    "        print (\"Recall: \" + str(recall))\n",
    "        print (\"Confusion Matrix\")\n",
    "        print (conf_mat)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "# Extract HOG features from an image and (optionally) show the features superimposed on it \n",
    "def extractHOG(inputimg, showHOG=False): \n",
    "    \n",
    "    # convert image to single-channel, grayscale\n",
    "    image = color.rgb2gray(inputimg)\n",
    "\n",
    "    #extract HOG features\n",
    "    if showHOG:\n",
    "        fd, hog_image = feature.hog(image, orientations=36, \n",
    "                                    pixels_per_cell=(16, 16),\n",
    "                                    cells_per_block=(2, 2), \n",
    "                                    visualise=showHOG)\n",
    "    else:\n",
    "        fd = feature.hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                         cells_per_block=(1, 1), visualise=showHOG)\n",
    "    if(showHOG):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "        ax1.set_adjustable('box-forced')\n",
    "        # Rescale histogram for better display\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 0.02))\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "        ax1.set_adjustable('box-forced')\n",
    "        plt.show()\n",
    "    return fd\n",
    "\n",
    "\n",
    "# Load a dataset (Data, Labels) from a folder.\n",
    "# Return data (HOGs, Class) and image list (as image file ames on disk)\n",
    "def load_dataset_from_folder(root_folder, rgb_folder, segmentation_folder):\n",
    "            \n",
    "    HOGs_list = []\n",
    "    Cs_list = []    \n",
    "    image_list = []\n",
    "    if os.path.exists(root_folder):\n",
    "        class_folders = next(os.walk(root_folder))[1]\n",
    "        class_folders.sort()\n",
    "        print(\"[INFO] Found \" + str(len(class_folders)) + \" class folders\")\n",
    "        print(class_folders)\n",
    "        sys.stdout.flush()\n",
    "        tot_classes = len(class_folders)\n",
    "        #used to resize the images\n",
    "        image_size = (128, 128)\n",
    "        class_list = range(tot_classes)\n",
    "        for class_folder,this_class in zip(class_folders,class_list):\n",
    "            print(\"\\n[INFO] Processing folder \" + class_folder)\n",
    "            sys.stdout.flush()\n",
    "            current_gesture_folder_rgb = root_folder + class_folder + \"/\" + rgb_folder + \"/*.jpg\"\n",
    "            current_gesture_folder_segmentation = root_folder + class_folder + \"/\" + segmentation_folder + \"/*.png\"\n",
    "            allfiles_imgs = glob.glob(current_gesture_folder_rgb)\n",
    "            allfiles_masks = glob.glob(current_gesture_folder_segmentation)\n",
    "            #for each image/mask pair\n",
    "            line_percentage_cnt = 0\n",
    "            for file_img,mask_img in zip(allfiles_imgs,allfiles_masks):\n",
    "                # Print completion percentage\n",
    "                sys.stdout.write('\\r')\n",
    "                progress_bar_msg = \"[%-100s] %d%% \" + str(line_percentage_cnt) + \"/\" + str(len(allfiles_imgs))\n",
    "                update_step = int( (float(100)/float(len(allfiles_imgs))) * float(line_percentage_cnt) )\n",
    "                sys.stdout.write(progress_bar_msg % ('='*update_step, update_step))\n",
    "                sys.stdout.flush()\n",
    "                img = io.imread(file_img)\n",
    "                mask = io.imread(mask_img)\n",
    "                mask = 255 - mask\n",
    "                img *= mask\n",
    "                # you can see the segmented image using:\n",
    "                #io.imshow(img)\n",
    "                #io.show()\n",
    "                \n",
    "                feat = extractHOG(transform.resize(img, image_size))\n",
    "                HOGs_list.append(feat)\n",
    "                Cs_list.append(this_class)\n",
    "                image_list.append(file_img)\n",
    "                line_percentage_cnt += 1\n",
    "        print(\"[INFO] Loaded data in. Number of samples: \"+ str(len(image_list)))\n",
    "    else:\n",
    "        print(\"[ERROR] Folder \" + root_folder + \" does not exist!\")\n",
    "        print(\"[ERROR] Have you run the setup cell?\")\n",
    "        sys.stdout.flush()\n",
    "        exit()\n",
    "        \n",
    "\n",
    "    HOGs = np.array(HOGs_list)\n",
    "    Cs = np.array(Cs_list)\n",
    "    return HOGs, Cs, image_list\n",
    "\n",
    "\n",
    "        \n",
    "# Class to store parameters of an SVM\n",
    "class SVMparameters:\n",
    "\n",
    "    def __init__(self, k='rbf', c='1', g='0.1', d=1):\n",
    "        self.kernel = k\n",
    "        self.C = c\n",
    "        self.gamma=g\n",
    "        self.degree = g\n",
    "\n",
    "    def setkernel(self, k):\n",
    "        self.kernel = k\n",
    "\n",
    "    def setgamma(self, g):\n",
    "        self.gamma = g\n",
    "\n",
    "    def setc(self, c):\n",
    "        self.C = c\n",
    "\n",
    "    def setdegree(self,d):\n",
    "        self.degree = d\n",
    "    \n",
    "    def printconfig(self):\n",
    "        print(\"Kernel: \" + self.kernel)\n",
    "        if self.kernel is \"poly\":\n",
    "            print(\"Degree: \" + str(self.degree))\n",
    "        print(\"C: \" + str(self.C))\n",
    "        print(\"Gamma: \" + str(self.gamma))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 141 class folders\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "\n",
    "\n",
    "### Run only if you have no data and need to do all the preprocessing in one go ###\n",
    "\n",
=======
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "train_folders = next(os.walk(\"KDEF/\"))[1]\n",
    "train_folders.sort()\n",
    "print(\"[INFO] Found \" + str(len(train_folders)) + \" class folders\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "image_list = []\n",
    "HOGs_list = []\n",
    "Cs_list = [] \n",
    "\n",
    "image_size = (128, 128)\n",
    "                \n",
<<<<<<< HEAD
    "tot_classes = len(train_folders)\n",
    "class_list = range(tot_classes)\n",
    "    \n",
    "face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "\n",
    "# AF = afraid\n",
    "# AN = angry\n",
    "# DI = disgusted \n",
    "# HA = happy \n",
    "# NE = neutral \n",
    "# SA = sad\n",
    "# SU = surprised\n",
    "\n",
    "\n",
=======
    "\n",
    "face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "emotion_list = ['AFS.JPG', 'ANS.JPG', 'DIS.JPG', 'HAS.JPG', 'NES.JPG', 'SAS.JPG', 'SUS.JPG']\n",
    "\n",
    "emotion_num = [0,1,2,3,4,5,6]\n",
    "\n",
    "ffile = []\n",
    "line_percentage_cnt = 0\n",
    "\n",
    "for class_folder,this_class in zip(train_folders,class_list):\n",
    "#     print(\"\\n[INFO] Processing folder \" + class_folder)\n",
    "    sys.stdout.flush()\n",
    "  \n",
    "    \n",
    "    ffile = os.listdir('KDEF/' + class_folder)\n",
    "\n",
    "    for i in ffile:\n",
    "        \n",
    "        for j in range(len(emotion_list)):\n",
    "\n",
    "            if i.endswith(emotion_list[j]): #afraid\n",
    "                \n",
    "                Cs_list.append(emotion_num[j])\n",
    "            \n",
    "                \n",
    "                directory = 'KDEF/train/' + emotion_list[j][0:3] \n",
    "                \n",
    "                RGB_subdir = 'RGB/'\n",
    "                segmented_subdir = 'segmented/'\n",
    "                \n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                     \n",
    "                if not os.path.exists(directory + \"/\" + RGB_subdir):\n",
    "                    os.makedirs(directory + \"/\" + RGB_subdir)\n",
    "                \n",
    "              \n",
    "                tmp_img = io.imread(\"KDEF/\" + class_folder + \"/\" + i)\n",
    "\n",
    "                \n",
    "            \n",
    "#                 sys.stdout.write('\\r')\n",
    "#                 progress_bar_msg = \"[%-100s] %d%% \" + str(line_percentage_cnt) + \"/\" + str(len(class_folder))\n",
    "#                 update_step = int( (float(100)/float(len(allfiles_imgs))) * float(line_percentage_cnt) )\n",
    "#                 sys.stdout.write(progress_bar_msg % ('='*update_step, update_step))\n",
    "#                 sys.stdout.flush()\n",
    "\n",
    "#                 feat = extractHOG(img_resize)\n",
    "        \n",
    "#                 HOGs_list.append(feat)\n",
    "    #             Cs_list.append(this_class)\n",
    "#                 image_list.append(tmp_img)\n",
    "\n",
    "#                 img = cv2.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "                gray = cv2.cvtColor(tmp_img,cv2.COLOR_BGR2GRAY)\n",
<<<<<<< HEAD
    "#                 ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        \n",
    "        \n",
    "                ###########################\n",
    "\n",
    "                gray = cv2.cvtColor(tmp_img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "                faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "                for (x,y,w,h) in faces:\n",
    "\n",
    "\n",
    "                    if h > 200 and w > 200:\n",
    "\n",
    "                        cv2.rectangle(tmp_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "                        roi_gray = gray[y:y+h, x:x+w]\n",
    "                        roi_color = tmp_img[y:y+h, x:x+w]\n",
    "                        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "                    #                     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "                        for (ex,ey,ew,eh) in eyes:\n",
    "                            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "                        img_resize = tmp_img[y:y+h,x:x+w]\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
=======
    "                ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        \n",
    "        \n",
    "                ###########################\n",
    "   \n",
    "                faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "                for (x,y,w,h) in faces:\n",
    "                    cv2.rectangle(tmp_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                    roi_gray = gray[y:y+h, x:x+w]\n",
    "                    roi_color = tmp_img[y:y+h, x:x+w]\n",
    "                    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "#                     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "                    for (ex,ey,ew,eh) in eyes:\n",
    "                        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "                img_resize = tmp_img[y:y+h,x:x+w]\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "\n",
    "                \n",
    "                io.imsave(directory + \"/\" + RGB_subdir + i, img_resize)\n",
    "                \n",
    "                ##########################\n",
    "\n",
    "# #                 # noise removal\n",
    "#                 kernel = np.ones((3,3),np.uint8)\n",
    "#                 opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "# #                 # sure background area\n",
    "#                 sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "\n",
    "# #                 # Finding sure foreground area\n",
    "#                 dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "#                 ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "\n",
    "# #                 # Finding unknown region\n",
    "#                 sure_fg = np.uint8(sure_fg)\n",
    "#                 mask_img = cv2.subtract(sure_bg,sure_fg)\n",
    "\n",
    "#                 if not os.path.exists(directory + \"/\" + segmented_subdir):\n",
    "#                     os.makedirs(directory + \"/\" + segmented_subdir)\n",
    "                    \n",
    "# #               #  cv2.imwrite('KDEF/test.png', mask_img)\n",
    "#                 io.imsave(directory + \"/\" + segmented_subdir + i, mask_img)\n",
    "\n",
    "#                 mask = 255 - mask_img\n",
    "#                 gray *= mask \n",
<<<<<<< HEAD
    "                feat = extractHOG(transform.resize(img_resize, image_size))\n",
=======
    "                feat = extractHOG(transform.resize(gray, image_size))\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "\n",
    "                HOGs_list.append(feat)\n",
    "#                 Cs_list.append(this_class)\n",
    "                image_list.append(img_resize)\n",
    "                \n",
    "\n",
    "        \n",
    "HOGs = np.array(HOGs_list)\n",
    "Cs = np.array(Cs_list)\n",
    "\n",
<<<<<<< HEAD
    "pickle.dump( HOGs, open( \"hogs.p\", \"wb\" ) )\n",
    "pickle.dump( Cs, open( \"cs.p\", \"wb\" ) )\n",
    "pickle.dump( image_list, open( \"images.p\", \"wb\" ) )\n",
    "\n",
    "\n",
    "\n",
=======
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "    \n",
    "#     return HOGs, Cs, image_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run to avoid regenerating all the data. Loads preprocessed data from disk ### \n",
    "\n",
    "HOGs = pickle.load( open( \"hogs.p\", \"rb\" ) )\n",
    "Cs = pickle.load( open( \"cs.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980,)\n",
      "(980, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Use only to visually inspect the HOGs features ###\n",
    "\n",
    "# HOGs = np.array(HOGs_list)\n",
    "# Cs = np.array(Cs_list)\n",
=======
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HOGs = np.array(HOGs_list)\n",
    "Cs = np.array(Cs_list)\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "\n",
    "print Cs.shape\n",
    "print HOGs.shape\n",
    "\n",
<<<<<<< HEAD
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "model_1 = model.fit_transform(HOGs) \n",
    "\n",
    "\n",
    "# colors = np.random.rand(N)\n",
    "# area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radiuses\n",
    "\n",
    "plt.scatter(model_1[:,0], model_1[:,1], c=Cs, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print Cs[0:10]\n",
    "    \n",
    "\n",
=======
    "print HOGs[0:10]\n",
    "print Cs[0:10]\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "# print image_list"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 42,
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Number of folds: 1\n",
      "Train-set size: 98\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.122448979592(12/98)\n",
      "Precision: 0.0174927113703\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0 15  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0]\n",
      " [ 0 21  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0]\n",
      " [ 0 14  0  0  0  0  0]\n",
      " [ 0 16  0  0  0  0  0]]\n",
      "******\n",
      "******\n",
      "Number of folds: 2\n",
      "Train-set size: 196\n",
      "Test-set size: 98\n",
<<<<<<< HEAD
      "(RW) Accuracy: 0.255102040816(25/98)\n",
      "Precision: 0.0871148459384\n",
      "Recall: 0.258928571429\n",
      "Confusion Matrix\n",
      "[[ 0  5  0  0  0  0 10]\n",
      " [ 0 12  0  0  0  0  0]\n",
      " [ 0 19  0  0  0  0  2]\n",
      " [ 0  8  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  2]\n",
      " [ 0 11  0  0  0  0  3]\n",
      " [ 0  3  0  0  0  0 13]]\n",
=======
      "(RW) Accuracy: 0.19387755102(19/98)\n",
      "Precision: 0.0578231292517\n",
      "Recall: 0.193452380952\n",
      "Confusion Matrix\n",
      "[[ 0  7  0  0  0  0  8]\n",
      " [ 0  8  0  0  0  0  4]\n",
      " [ 0 19  0  0  0  0  2]\n",
      " [ 0  5  0  0  0  0  3]\n",
      " [ 0  4  0  0  0  0  8]\n",
      " [ 0  8  0  0  0  0  6]\n",
      " [ 0  5  0  0  0  0 11]]\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
      "******\n",
      "******\n",
      "Number of folds: 3\n",
      "Train-set size: 294\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.163265306122(16/98)\n",
      "Precision: 0.0233236151603\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  8]\n",
      " [ 0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0 16]]\n",
      "******\n",
      "******\n",
      "Number of folds: 4\n",
      "Train-set size: 392\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.163265306122(16/98)\n",
      "Precision: 0.0233236151603\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  8]\n",
      " [ 0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0 16]]\n",
      "******\n",
      "******\n",
      "Number of folds: 5\n",
      "Train-set size: 490\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.122448979592(11/98)\n",
      "Precision: 0.0174927113703\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0  0 15  0  0]\n",
      " [ 0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0 21  0  0]\n",
      " [ 0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0 14  0  0]\n",
      " [ 0  0  0  0 16  0  0]]\n",
      "******\n",
      "******\n",
      "Number of folds: 6\n",
      "Train-set size: 588\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.0816326530612(8/98)\n",
      "Precision: 0.0116618075802\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0 15  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 21  0  0  0]\n",
      " [ 0  0  0  8  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 14  0  0  0]\n",
      " [ 0  0  0 16  0  0  0]]\n",
      "******\n",
      "******\n",
      "Number of folds: 7\n",
      "Train-set size: 686\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.0816326530612(8/98)\n",
      "Precision: 0.0116618075802\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0 15  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 21  0  0  0]\n",
      " [ 0  0  0  8  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 14  0  0  0]\n",
      " [ 0  0  0 16  0  0  0]]\n",
      "******\n",
      "******\n",
      "Number of folds: 8\n",
      "Train-set size: 784\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.0816326530612(8/98)\n",
      "Precision: 0.0116618075802\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0 15  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 21  0  0  0]\n",
      " [ 0  0  0  8  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 14  0  0  0]\n",
      " [ 0  0  0 16  0  0  0]]\n",
      "******\n",
      "******\n",
      "Number of folds: 9\n",
      "Train-set size: 882\n",
      "Test-set size: 98\n",
      "(RW) Accuracy: 0.0816326530612(8/98)\n",
      "Precision: 0.0116618075802\n",
      "Recall: 0.142857142857\n",
      "Confusion Matrix\n",
      "[[ 0  0  0 15  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 21  0  0  0]\n",
      " [ 0  0  0  8  0  0  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0 14  0  0  0]\n",
      " [ 0  0  0 16  0  0  0]]\n",
      "******\n"
     ]
<<<<<<< HEAD
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelali/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/aelali/anaconda/lib/python2.7/site-packages/matplotlib/tight_layout.py:222: UserWarning: tight_layout : falling back to Agg renderer\n",
      "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Train your classifier, K-fold only ###\n",
    "\n",
=======
    }
   ],
   "source": [
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "    \n",
    "# Split the data in K-fold\n",
<<<<<<< HEAD
    "num_of_folds = 10\n",
=======
    "num_of_folds = 5\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "test_ratio = 1.0 / float(num_of_folds) #reserve one for testing\n",
    "HOGs_train, HOGs_test, Cs_train, Cs_test = train_test_split(HOGs, Cs, test_size=test_ratio, random_state=42)\n",
    "fold_size = HOGs.shape[0]/num_of_folds #size of individual fold\n",
    "\n",
    "\n",
    "\n",
    "all_acc = []\n",
    "all_prec = []\n",
    "all_rec = []\n",
<<<<<<< HEAD
    "clf_pt1 = svm.SVC(decision_function_shape='ovo') # ovo = one-vs-one, for all classes\n",
=======
    "clf_pt1 = svm.SVC(decision_function_shape='ova') # ovo = one-vs-one, for all classes\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "\n",
    "#grow the training set adding a fold each time, then test the perfomances\n",
    "for i in range(1, num_of_folds):\n",
    "\n",
    "    print(\"******\")\n",
    "    print(\"Number of folds: \" + str(i))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    ## TRAINING ##\n",
    "\n",
    "    #build training set\n",
    "    HOGs_growing_train = HOGs_train[0:(i*fold_size),:]\n",
    "    Cs_growing_train = Cs_train[0:(i*fold_size)]\n",
    "        \n",
    "    print(\"Train-set size: \" + str(len(Cs_growing_train)))\n",
    "    print(\"Test-set size: \" + str(len(Cs_test)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #train multi-class SVM\n",
<<<<<<< HEAD
    "    clf_pt1 = svm.SVC(decision_function_shape='ovo')\n",
=======
    "    clf_pt1 = svm.SVC(decision_function_shape='ova')\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "    clf_pt1.fit(HOGs_growing_train, Cs_growing_train.ravel())\n",
    "\n",
    "    ## TESTING ##\n",
    "\n",
    "    # test\n",
    "    Cs_predicted = clf_pt1.predict(HOGs_test)\n",
    "\n",
    "    # compute stats    \n",
    "    accuracy, precision, recall = compute_scores(Cs_predicted, Cs_test, verbose=True)\n",
    "    all_acc.append(accuracy)\n",
    "    all_prec.append(precision)\n",
    "    all_rec.append(recall)\n",
    "\n",
    "    print(\"******\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# now plot\n",
    "x = range(1, num_of_folds)\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Performances\")\n",
    "\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.plot(x, all_acc)\n",
    "ax1.locator_params(nbins=num_of_folds-1)\n",
    "\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.set_title(\"Precision\")\n",
    "ax2.plot(x, all_prec)\n",
    "ax2.locator_params(nbins=num_of_folds-1)\n",
    "\n",
    "ax3 = fig.add_subplot(313)\n",
    "ax3.set_title(\"Recall\")\n",
    "ax3.plot(x, all_rec)\n",
    "ax3.locator_params(nbins=num_of_folds-1)\n",
    "\n",
    "#minimise subplots overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "(RW) Accuracy: 0.761904761905(95/126)\n",
      "Precision: 0.770817709235\n",
      "Recall: 0.761904761905\n",
      "Confusion Matrix\n",
      "[[11  2  0  0  1  1  3]\n",
      " [ 1 15  1  1  0  0  0]\n",
      " [ 1  0 16  0  0  1  0]\n",
      " [ 0  0  0 18  0  0  0]\n",
      " [ 2  2  0  0 12  0  2]\n",
      " [ 2  3  0  0  3 10  0]\n",
      " [ 2  1  0  0  1  0 14]]\n",
      "K-fold iteration 1/8 -- (rw) accuracy:0.761904761905\n",
      "(RW) Accuracy: 0.730158730159(91/126)\n",
      "Precision: 0.756096677494\n",
      "Recall: 0.730158730159\n",
      "Confusion Matrix\n",
      "[[17  1  0  0  0  0  0]\n",
      " [ 1 14  2  0  1  0  0]\n",
      " [ 1  2 14  0  0  1  0]\n",
      " [ 1  0  0 17  0  0  0]\n",
      " [ 3  0  0  1 12  2  0]\n",
      " [ 4  1  1  1  3  7  1]\n",
      " [ 4  0  0  0  3  0 11]]\n",
      "K-fold iteration 2/8 -- (rw) accuracy:0.730158730159\n",
      "(RW) Accuracy: 0.65873015873(83/126)\n",
      "Precision: 0.688105063202\n",
      "Recall: 0.65873015873\n",
      "Confusion Matrix\n",
      "[[ 8  3  2  1  0  1  3]\n",
      " [ 0 11  5  0  2  0  0]\n",
      " [ 1  1 13  0  1  1  1]\n",
      " [ 1  1  1 12  1  0  2]\n",
      " [ 0  1  0  0 16  1  0]\n",
      " [ 0  2  0  0  6  9  1]\n",
      " [ 1  0  0  0  3  0 14]]\n",
      "K-fold iteration 3/8 -- (rw) accuracy:0.65873015873\n",
      "(RW) Accuracy: 0.587301587302(74/126)\n",
      "Precision: 0.634067311135\n",
      "Recall: 0.587301587302\n",
      "Confusion Matrix\n",
      "[[12  2  0  0  0  1  3]\n",
      " [ 2 11  1  0  2  1  1]\n",
      " [ 2  3 10  1  1  1  0]\n",
      " [ 4  1  1 11  0  1  0]\n",
      " [ 3  0  0  0  7  6  2]\n",
      " [ 5  1  0  0  1 10  1]\n",
      " [ 2  1  0  0  1  1 13]]\n",
      "K-fold iteration 4/8 -- (rw) accuracy:0.587301587302\n",
      "(RW) Accuracy: 0.756302521008(90/119)\n",
      "Precision: 0.761021615124\n",
      "Recall: 0.756302521008\n",
      "Confusion Matrix\n",
      "[[11  0  1  0  2  0  3]\n",
      " [ 1 10  1  0  2  2  1]\n",
      " [ 2  0 14  1  0  0  0]\n",
      " [ 1  1  0 15  0  0  0]\n",
      " [ 0  0  0  0 14  2  1]\n",
      " [ 0  2  1  0  2 12  0]\n",
      " [ 1  0  0  0  1  1 14]]\n",
      "K-fold iteration 5/8 -- (rw) accuracy:0.756302521008\n",
      "(RW) Accuracy: 0.663865546218(79/119)\n",
      "Precision: 0.687844611529\n",
      "Recall: 0.663865546218\n",
      "Confusion Matrix\n",
      "[[ 9  1  0  1  1  3  2]\n",
      " [ 2 11  1  0  3  0  0]\n",
      " [ 0  3 13  0  0  1  0]\n",
      " [ 1  1  1 14  0  0  0]\n",
      " [ 0  3  0  0 12  2  0]\n",
      " [ 4  1  0  0  2 10  0]\n",
      " [ 3  0  0  0  1  3 10]]\n",
      "K-fold iteration 6/8 -- (rw) accuracy:0.663865546218\n",
      "(RW) Accuracy: 0.621848739496(74/119)\n",
      "Precision: 0.62190446354\n",
      "Recall: 0.621848739496\n",
      "Confusion Matrix\n",
      "[[ 5  2  1  0  1  3  5]\n",
      " [ 0 10  2  0  3  2  0]\n",
      " [ 0  4 10  0  0  2  1]\n",
      " [ 1  0  0 15  0  0  1]\n",
      " [ 2  1  0  0 12  1  1]\n",
      " [ 2  1  0  1  2 10  1]\n",
      " [ 2  1  1  0  1  0 12]]\n",
      "K-fold iteration 7/8 -- (rw) accuracy:0.621848739496\n",
      "(RW) Accuracy: 0.563025210084(66/119)\n",
      "Precision: 0.584823666309\n",
      "Recall: 0.563025210084\n",
      "Confusion Matrix\n",
      "[[ 6  2  1  0  3  4  1]\n",
      " [ 2  9  1  0  3  2  0]\n",
      " [ 1  4 11  0  0  1  0]\n",
      " [ 2  0  0 15  0  0  0]\n",
      " [ 2  2  0  0  9  3  1]\n",
      " [ 5  2  3  0  2  5  0]\n",
      " [ 1  1  0  0  3  0 12]]\n",
      "K-fold iteration 8/8 -- (rw) accuracy:0.563025210084\n",
      "K-fold averaged performances.\n",
      "(RW) Accuracy: 0.667892156863\n",
      "Precision: 0.688085139696\n",
      "Recall: 0.667892156863\n",
      "[INFO] Finished part 3 (cross validation)\n"
=======
      "(379, 379, 3)\n"
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
     ]
    }
   ],
   "source": [
    "\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "### Train your classifer using k-fold cross-validaton ###\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import pickle\n",
    "# if (sys.version_info > (3, 0)):\n",
    "#     # Python 3 detected\n",
    "#     import configparser as cp\n",
    "# else:\n",
    "#     # Python 2 detected\n",
    "#     import ConfigParser as cp\n",
    "\n",
    "# # make sure all defs are executed before start working in this cell\n",
    "# if 'setup_run' not in locals():\n",
    "#     print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "# if 'HOGs' not in locals() or 'Cs' not in locals():\n",
    "#     print('Trying to load training data from file')\n",
    "#     HOGs = pickle.load(open(config.get('Train','features_complete'),\"rb\"))\n",
    "#     Cs = pickle.load(open(config.get('Train','labels_complete'),\"rb\"))\n",
    "\n",
    "# if 'best_svm_params' not in locals() or 'best_svm_params' not in vars() or 'best_svm_params' not in globals():\n",
    "#     print('didn\\'t find best_svm_params')\n",
    "    \n",
    "# try:\n",
    "#     best_svm_params\n",
    "# except:\n",
    "#     print('best_svm_params not defined. Trying to load from file')\n",
    "#     best_svm_params = pickle.load(open(config.get('Train', 'best_model_params_pickle_out'), 'rb'))\n",
    "    \n",
    "# # Part 3: cross-validation\n",
    "# print(\"[INFO] Executing part 3 (cross validation)\")\n",
    "\n",
    "acc = 0\n",
    "prec = 0\n",
    "rec = 0\n",
    "\n",
    "# create k-folds indeces\n",
    "num_of_test_folds = 8\n",
    "skf = StratifiedKFold(Cs.ravel(), n_folds=num_of_test_folds)\n",
    "\n",
    "# for each fold, set the current as test, and the rest as train\n",
    "kfold_cnt = 1\n",
    "for train_index, test_index in skf:\n",
    "\n",
    "    # build test/train sets\n",
    "    HOGs_train = HOGs[train_index, :]\n",
    "    HOGs_test = HOGs[test_index, :]\n",
    "    Cs_train = Cs[train_index]\n",
    "    Cs_test = Cs[test_index]\n",
    "\n",
    "    # train\n",
    "    # using best configuration from previous cell (k-folds, k=8):\n",
    "    # Best configuration:\n",
    "    # Kernel: poly\n",
    "    # Degree: 3.0\n",
    "    # C: 0.1\n",
    "    # Gamma: 100.0\n",
    "    clf_pt3 = svm.SVC(decision_function_shape='ova',\n",
    "                      kernel=\"poly\",\n",
    "                      degree=3.1,\n",
    "                      C=0.1,\n",
    "                      gamma=100)\n",
    "    \n",
    "    \n",
    "    # Kernel: poly\n",
    "# Degree: 3.0\n",
    "# C: 0.1\n",
    "# Gamma: 100.0\n",
    "\n",
    "#     clf_pt3 = svm.SVC(decision_function_shape='ovo')\n",
    "    \n",
    "                        \n",
    "    \n",
    "    clf_pt3.fit(HOGs_train, Cs_train.ravel())\n",
    "\n",
    "    # test\n",
    "    Cs_predicted = clf_pt3.predict(HOGs_test)\n",
    "\n",
    "    \n",
    "    # compute stats\n",
    "    accuracy, precision, recall = compute_scores(Cs_predicted, Cs_test, verbose=True)\n",
    "    print(\"K-fold iteration \" + str(kfold_cnt) + \"/\" + str(num_of_test_folds) + \" -- (rw) accuracy:\" + str(accuracy))\n",
    "    acc += accuracy\n",
    "    prec += precision\n",
    "    rec += recall\n",
    "    #increment count\n",
    "    kfold_cnt += 1\n",
    "\n",
    "#average stats\n",
    "acc /= num_of_test_folds\n",
    "prec /= num_of_test_folds\n",
    "rec /= num_of_test_folds\n",
    "\n",
    "\n",
    "print(\"K-fold averaged performances.\")\n",
    "print(\"(RW) Accuracy: \" + str(acc))\n",
    "print(\"Precision: \" + str(prec))\n",
    "print(\"Recall: \" + str(rec))\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Part 3: cross-validation\n",
    "print(\"[INFO] Finished part 3 (cross validation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelali/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### SCRATCH ###\n",
    "\n",
    "image_size = (128, 128)\n",
    "haar_img = get_haar_features(\"sad.png\")\n",
    "\n",
    "io.imshow(haar_img)\n",
    "io.show()\n",
    "\n",
    "feat = extractHOG(transform.resize(haar_img, image_size))\n",
    "clf_pt3.predict(feat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'roi_color' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-7b8906c4cf08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mget_haar_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tmpframe.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-7b8906c4cf08>\u001b[0m in \u001b[0;36mget_haar_features\u001b[0;34m(input_img)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroi_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'roi_color' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### SCRATCH ###\n",
    "\n",
    "def get_haar_features(input_img):\n",
    "\n",
    "    io_img = io.imread(input_img)\n",
    "    face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "\n",
    "    gray = cv2.cvtColor(io_img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "\n",
    "        if h > 200 and w > 200:\n",
    "\n",
    "            cv2.rectangle(io_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = io_img[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        #                     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "            for (ex,ey,ew,eh) in eyes:\n",
    "                cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "                \n",
    "#             img_resize = io_img[y:y+h,x:x+w]\n",
    "            \n",
    "            \n",
    "    \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "   \n",
    "    return roi_color\n",
    "   \n",
    "\n",
    "\n",
    "def predict(ffile):\n",
    "\n",
    "    image_size = (128, 128)\n",
    "    haar_img = get_haar_features(ffile)\n",
    "    feat = extractHOG(transform.resize(haar_img, image_size))\n",
    "#     accuracy, precision, recall = compute_scores(clf_pt3.predict(feat), label, verbose=True)\n",
    "    \n",
    "    return clf_pt3.predict(feat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get_haar_features(\"tmpframe.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelali/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Generate prediction from video frame ###\n",
    "\n",
    "from window import window_data\n",
    "\n",
    "## The skeleton of a solution\n",
    "\n",
    "import window\n",
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from IPython.core.display import HTML\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)\n",
    "import pandas as pd\n",
    "import time\n",
    "import cv2\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "# load the wave file and normalise\n",
    "# load \"data/rub_1.wav\" and \"data/rub_2.wav\"\n",
    "\n",
    "\n",
    "\n",
    "def get_haar_features(input_img):\n",
    "\n",
    "    io_img = io.imread(input_img)\n",
    "    face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "\n",
    "    gray = cv2.cvtColor(io_img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "\n",
    "        if h > 200 and w > 200:\n",
    "\n",
    "            cv2.rectangle(io_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = io_img[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        #                     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "            for (ex,ey,ew,eh) in eyes:\n",
    "                cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "                \n",
    "#             img_resize = io_img[y:y+h,x:x+w]\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return roi_color\n",
    "\n",
    "def load_wave(fname):\n",
    "    # load and return a wave file\n",
    "    sr, wave = scipy.io.wavfile.read(fname)\n",
    "    return wave/32768.0\n",
    "\n",
    "\n",
    "def emoji_classify():\n",
    "\n",
    "    rub_1 = load_wave(\"emoji.wav\")[:,0]\n",
    "    rub_2 = load_wave(\"anythingelse.wav\")[:,0]\n",
    "\n",
    "    rub_1_features = window.window_data(rub_1, 120)\n",
    "    rub_2_features = window.window_data(rub_2, 120)\n",
    "\n",
    "    rub_1_labels = np.zeros(len(rub_1_features,))\n",
    "    rub_2_labels = np.ones(len(rub_2_features,))\n",
    "\n",
    "    rub_features = np.vstack([rub_1_features, rub_2_features])\n",
    "    rub_labels = np.hstack([rub_1_labels, rub_2_labels])\n",
    "\n",
    "    rubfft_features =  np.abs(np.fft.fft(rub_features))\n",
    "    rubfft_train_features, rubfft_test_features, rub_train_labels, rub_test_labels = sklearn.cross_validation.train_test_split(\n",
    "        rubfft_features, rub_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "    rub_train_features, rub_test_features, rub_train_labels, rub_test_labels = sklearn.cross_validation.train_test_split(\n",
    "        rub_features, rub_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "    #print rub_train_features.shape, rub_train_labels.shape\n",
    "\n",
    "    svm = sklearn.svm.SVC(gamma=0.1, C=100)\n",
    "    svm.fit(rub_train_features, rub_train_labels)\n",
    "\n",
    "\n",
    "    return svm\n",
    "\n",
    "# svm = emoji_classify()\n",
    "\n",
    "CHUNK = 1024 \n",
    "FORMAT = pyaudio.paInt16 #paInt8\n",
    "CHANNELS = 2 \n",
    "RATE = 44100 #sample rate\n",
    "RECORD_SECONDS = 10\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n",
    "\n",
    "recorded = load_wave(\"output.wav\")[:,0]\n",
    "\n",
    "recorded_features = window.window_data(recorded, 120)\n",
    "\n",
    "recorded_features = np.vstack(recorded_features)\n",
    "recorded_labels = svm.predict(recorded_features)\n",
    "\n",
    "def readFacialExpression():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH);\n",
    "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT); \n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    out = cv2.VideoWriter('output.avi',fourcc, 25.0, (int(w),int(h)))\n",
    "    #out = cv2.VideoWriter('output.avi', -1, 20.0, (640,480))\n",
    "\n",
    "    timeInit = time.time()\n",
    "    # only record for 200 ms \n",
    "    while(cap.isOpened() & (500 > ((time.time() - timeInit)*1000))):\n",
    "        ret, frame = cap.read()\n",
    "        if ret==True:\n",
    "            #frame = cv2.flip(frame,0)\n",
    "            # write the flipped frame\n",
    "            out.write(frame)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "#print recorded_labels\n",
    "\n",
    "#window_index to decide whether to open the window or not\n",
    "window_index = 0\n",
    "\n",
    "for i in range(len(recorded_labels.tolist())):\n",
    "    if (recorded_labels[i] == 0 ):\n",
    "        window_index = 1\n",
    "\n",
    "if (window_index == 1):\n",
    "    readFacialExpression()\n",
    "\n",
    "\n",
    "def predict(ffile):\n",
    "\n",
    "    image_size = (128, 128)\n",
    "    haar_img = get_haar_features(ffile)\n",
    "    feat = extractHOG(transform.resize(haar_img, image_size))\n",
    "#     accuracy, precision, recall = compute_scores(clf_pt3.predict(feat), label, verbose=True)\n",
    "\n",
    "    io.imshow(haar_img)\n",
    "    io.show()\n",
    "    return clf_pt3.predict(feat)\n",
    "\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# snapshot used as input to trained classifier (e.g., scikit.svm.predict(snapshot)). \n",
    "# returns label (happy, sad, neutral, angry)\n",
    "def classifyFacialExpression():\n",
    "    \n",
    "    readFacialExpression()\n",
    "#     HOGs_list = []\n",
    "#     Cs_list = []\n",
    "#     best_list = []\n",
    "    prediction = -1\n",
    "    cap = cv2.VideoCapture('output.avi')\n",
    "    \n",
    "#     print int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for i in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        pil_im = Image.fromarray(frame)\n",
    "        \n",
    "        \n",
    "    \n",
    "    pil_im.show()\n",
    "    \n",
    "    io.imsave(\"tmpframe.png\", pil_im)\n",
    "\n",
    "    prediction = predict(\"tmpframe.png\")\n",
    "    \n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "classifyFacialExpression()\n",
    "\n",
    "# In[73]:\n",
    "\n",
=======
    "face_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_eye.xml')\n",
    "# mouth_cascade = cv2.CascadeClassifier('FaceDetect/haarcascade_smile.xml')\n",
    "\n",
    "\n",
    "img = cv2.imread('KDEF/AF01/AF01AFS.JPG')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "#     mouth = mouth_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    \n",
    "#     for (ex,ey,ew,eh) in mouth:\n",
    "#         cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "        \n",
    "img_resize = img[y:y+h,x:x+w]\n",
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Cartoonizer ###\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# io.imsave(\"tmpframe.png\", pil_im)\n",
    "img_rgb2 = cv2.imread(\"tmpframe.png\")\n",
    "\n",
    "img_rgb = img_rgb2[300:700,400:600]\n",
    "\n",
    "\n",
    "def convert(img_rgb):\n",
    "    num_down = 2       # number of downsampling steps\n",
    "    num_bilateral = 7  # number of bilateral filtering steps\n",
    "\n",
    "    # # downsample image using Gaussian pyramid\n",
    "    img_color = img_rgb\n",
    "    for _ in xrange(num_down):\n",
    "        img_color = cv2.pyrDown(img_color)\n",
    "\n",
    "    # # repeatedly apply small bilateral filter instead of\n",
    "    # # applying one large filter\n",
    "    for _ in xrange(num_bilateral):\n",
    "        img_color = cv2.bilateralFilter(img_color, d=9,sigmaColor=9, sigmaSpace=7)\n",
    "\n",
    "    for _ in xrange(num_down):\n",
    "        img_color = cv2.pyrUp(img_color)\n",
    "\n",
    "\n",
    "    # # convert to grayscale and apply median blur\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "    img_blur = cv2.medianBlur(img_gray, 7)\n",
    "\n",
    "    # # detect and enhance edges\n",
    "    img_edge = cv2.adaptiveThreshold(img_blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "        cv2.THRESH_BINARY, blockSize=9, C=2)\n",
    "\n",
    "    # # convert back to color, bit-AND with color image\n",
    "    img_edge = cv2.cvtColor(img_edge, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    tmp = []\n",
    "    for i in range(img_edge.shape[0]):\n",
    "        tmp.append(img_color[i][:img_edge.shape[1]])\n",
    "    img_color = np.array(tmp)\n",
    "\n",
    "\n",
    "    img_cartoon = cv2.bitwise_and(img_color, img_edge)\n",
    "    r = 100.0 / img_cartoon.shape[1]\n",
    "    dim = (100, int(img_cartoon.shape[0] * r))\n",
    "\n",
    "    # perform the actual resizing of the image and show it\n",
    "    resized = cv2.resize(img_cartoon, dim, interpolation = cv2.INTER_AREA)\n",
    "    #cv2.imshow(\"resized\", resized)\n",
    "    height,width,depth = resized.shape\n",
    "    circle_img = np.zeros((height,width), np.uint8)\n",
    "    cv2.circle(circle_img,(width/2,height/2),40,1,thickness=-1)\n",
    "    masked_data = cv2.bitwise_and(resized, resized, mask=circle_img)\n",
    "    cv2.imshow(\"masked\", masked_data)\n",
    "    cv2.imwrite(str(emoji_index) + '.png',masked_data)\n",
    "\n",
    "# emoji_list = [index, label]\n",
    "\n",
    "\n",
    "## gives the image an index and  store the image with its label into database\n",
    "emoji_index = 1\n",
    "# emoji_index = emoji_index + 1\n",
    "emoji_label = 1 # it's a loop\n",
    "\n",
    "index_list = []\n",
    "label_list = []\n",
    "\n",
    "#get a new image and recognize whether the new image matches existing emojis\n",
    "if (len(label_list) > 0):\n",
    "    for i in range(len(label_list)):\n",
    "        if (emoji_label == label_list[i]):\n",
    "            Toshow = cv2.imread(str(index_list[i]) + '.png')\n",
    "            Toshow.imshow(); #show the image in emoji_list with corresponding emoji_index\n",
    "            cv2.waitKey()\n",
    "        else: \n",
    "            #emoji_list.append([emoji_index,emoji_label])\n",
    "            index_list.append(emoji_index)\n",
    "            label_list.append(emoji_label)\n",
    "            #emoji_index = emoji_index + 1\n",
    "            #emoji_label = emoji_label + 1\n",
    "            convert(img_rgb); #show this image\n",
    "            cv2.waitKey()\n",
    "else:\n",
    "    index_list.append(emoji_index)\n",
    "    label_list.append(emoji_label)\n",
    "    convert(img_rgb); #show this image\t\t\n",
    "    cv2.waitKey()\n",
    "\n",
    "print index_list\n",
    "print label_list\n"
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = io.imread(\"KDEF/AF01/AF01AFFL.JPG\")\n",
    "mask = io.imread(mask_img\n",
    "mask = 255 - mask\n",
    "img *= mask\n",
    "# you can see the segmented image using:\n",
    "#io.imshow(img)\n",
    "#io.show()\n",
    "feat = extractHOG(transform.resize(img, image_size))\n",
    "\n",
    "extractHOG(img, showHOG=True)"
>>>>>>> 65b641b5c1a34627b71b1d613d724c0d8f592fb6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
